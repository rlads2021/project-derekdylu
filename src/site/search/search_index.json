{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u53f0\u7063\u5347\u5927\u5b78\u8003\u8a66\u570b\u5beb\u60c5\u610f\u984c\u4f73\u4f5c\u8a5e\u983b\u76f8\u95dc\u7814\u7a76 \u00b6 \u7b2c\u4e09\u7d44 \u60f3\u4e0d\u5230\u7d44\u540d \u7d44\u54e1\uff1a\u76e7\u5fb7\u539f\u3001\u6731\u4fee\u5e73\u3001\u694a\u8212\u6674\u3001\u9673\u5b9b\u7469 Python Project \u00b6 import numpy as np import pandas as pd import gdown import os import re import seaborn as sns from matplotlib import pyplot as plt import statsmodels.formula.api as smf import torch import transformers import CwnSenseTagger #CwnSenseTagger.download() import CwnGraph #CwnGraph.download() from CwnGraph import CwnBase from ckiptagger import data_utils , construct_dictionary , WS , POS , NER #import DistilTag #DistilTag.download() ws = WS ( os . path . abspath ( os . path . join ( os . getcwd (), os . path . pardir )) + '/data' ) pos = POS ( os . path . abspath ( os . path . join ( os . getcwd (), os . path . pardir )) + '/data' ) ner = NER ( os . path . abspath ( os . path . join ( os . getcwd (), os . path . pardir )) + '/data' ) /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0. warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \" /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead. warnings.warn('`layer.add_variable` is deprecated and ' \u8b80\u53d6\u8207\u6574\u7406\u8cc7\u6599\u96c6 \u00b6 all_f = [] for file in os . listdir ( \"data_set/\" ): if file . endswith ( \".txt\" ): all_f . append ( file ) ''' print(\"list length = \", len(all_f)) print(all_f[2]) print(len(all_f[2])) ''' '\\nprint(\"list length = \", len(all_f))\\nprint(all_f[2])\\nprint(len(all_f[2]))\\n' sentence_list = [] sentence_list_type = [] sentence_list_year = [] for i in all_f : f = open ( \"data_set/\" + i ) sentence_list . append ( f . read ()) if \"GSAT\" in i : sentence_list_type . append ( \"GSAT\" ) else : sentence_list_type . append ( \"AST\" ) year = re . search ( \"\\_(...|..)\\_\" , i ) . group ( 1 ) sentence_list_year . append ( year ) all_list = pd . DataFrame ({ 'type' : sentence_list_type , 'year' : sentence_list_year , 'sentence' : sentence_list }) all_list .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type year sentence 0 GSAT 105 \u9019\u6642\uff0c\u4eba\u4e5f\u53ea\u80fd\u7b11\u4e86\u3002\\n\u8607\u8fea\u52d2\u7684\u72c2\u98a8\u6a6b\u6383\u5168\u81fa\uff0c\u5b83\u7834\u58de\u4e86\uff0c\u4e5f\u5275\u9020\u4e86\u3002\u81fa\u5317\u5e02\u7684\u5169\u500b\u90f5\u7b52\u5728\u4e00\u591c\u8086\u8650... 1 GSAT 107 \u79cb\u5929\u7e3d\u7d66\u4eba\u4e00\u7a2e\u856d\u745f\u4e4b\u611f\uff0c\u4f46\u6211\u537b\u5c0d\u79cb\u5929\u60c5\u6709\u7368\u937e\u3002\\n\u79cb\u98a8\u98af\u98af\uff0c\u6372\u8d77\u4e86\u5343\u5806\u843d\u8449\uff0c\u4e5f\u6372\u8d77\u4e86\u773e\u591a\u9077\u5ba2... 2 AST 101 \u5b83\uff0c\u873f\u8712\u904e\u571f\u58e4\u7684\u7e2b\u9699\uff1b\u5b83\uff0c\u5bc4\u8eab\u65bc\u5927\u6d77\u7684\u6e5b\u85cd\u3002\u5b83\uff0c\u6452\u6301\u8457\u5112\u8005\u300c\u539f\u6cc9\u6efe\u6efe\uff0c\u4e0d\u542b\u665d\u591c\u3002\u300d\u7684\u7cbe\u795e\u5954\u6d41... 3 GSAT 101 \u4eba\u751f\u662f\u4e00\u689d\u9577\u6cb3\uff0c\u552f\u6709\u5805\u786c\u7684\u5375\u77f3\u624d\u80fd\u6fc0\u76ea\u51fa\u7f8e\u9e97\u7684\u6c34\u82b1\uff0c\u4e5f\u552f\u6709\u4e00\u4efd\u9365\u800c\u4e0d\u820d\u7684\u771f\u60c5\u624d\u80fd\u62d3\u5c55\u751f\u547d\u7684\u5bec... 4 AST 103 \u6700\u8ca7\u7aae\u7684\u4eba\u4e0d\u662f\u6c92\u6709\u9322\u8ca1\uff0c\u800c\u662f\u6c92\u6709\u5922\u60f3\u3002\u6709\u4e86\u5922\u60f3\u5c31\u50cf\u662f\u6709\u4e86\u7f85\u76e4\u7684\u822a\u884c\uff1b\u5c31\u50cf\u662f\u6709\u4e86\u5fc3\u4e2d\u7684\u7f85\u99ac\uff0c\u53ea... ... ... ... ... 203 GSAT 110 \u5982\u679c\u6211\u6709\u4e00\u5ea7\u65b0\u51b0\u7bb1\uff0c\u90a3\u60f3\u5fc5\u662f\u6e05\u65b0\u53ef\u4eba\uff0c\u689d\u7406\u5206\u660e\u3002\u56e0\u70ba\u51b0\u85cf\u5728\u5176\u9593\u7684\uff0c\u4e0d\u50c5\u662f\u98df\u7269\u672c\u8eab\uff0c\u66f4\u860a\u542b\u80cc\u5f8c... 204 AST 101 \u4eba\u7e3d\u514d\u4e0d\u4e86\u81ea\u5df1\u4e00\u500b\u4eba\u7684\u3002\u6709\u4eba\u6015\u5bc2\u5bde\uff0c\u8aaa\uff1a\u300c\u5bc2\u5bde\uff0c\u96e3\u8010\u3002\u300d\u4ed6\u5011\u6015\u5b64\u7368\uff0c\u5b64\u7368\u8b93\u4ed6\u5011\u60f6\u6050\u3001\u5bb3\u6015\uff0c\u611f... 205 AST 103 \u6211\u7684\u7956\u7236\u5df2\u7d93\u9ad8\u9f61\u4e5d\u5341\u4e8c\u6b72\uff0c\u4ed6\u8eab\u908a\u7684\u4eba\u6b63\u4e00\u500b\u500b\u96e2\u4ed6\u800c\u53bb\u3002\u6709\u4e00\u6b21\u548c\u6211\u804a\u5929\u7684\u6642\u5019\uff0c\u6211\u5f9e\u4ed6\u53e3\u4e2d\u59d4\u5a49\u7684... 206 AST 105 \u300c\u9806\u98a8\u53ef\u4ee5\u822a\u884c\uff0c\u9006\u98a8\u53ef\u4ee5\u98db\u884c\u3002\u300d\u751f\u547d\uff0c\u662f\u4e00\u9023\u4e32\u7684\u529f\u8ab2\uff0c\u5176\u4e2d\u4e4b\u4e00\uff0c\u4fbf\u662f\u300c\u8209\u91cd\u82e5\u8f15\u300d\u7684\u5b78\u554f\u3002\u9806\u5883... 207 GSAT 102 \u97f3\u6a02\uff0c\u98c4\u63da\u65bc\u7a7a\u4e2d\uff0c\u90a3\u662f\u807d\u89ba\u7684\u6109\u5feb\uff1b\u9178\u751c\u82e6\u8fa3\uff0c\u649e\u64ca\u5473\u857e\uff0c\u90a3\u662f\u5473\u89ba\u7684\u6109\u5feb\uff1b\u300c\u6c99\u9dd7\u7fd4\u96c6\uff0c\u9326\u9c57\u6e38\u6cf3\u300d... 208 rows \u00d7 3 columns ckiptagger \u00b6 word_sentence_list = ws ( sentence_list ) pos_sentence_list = pos ( word_sentence_list ) entity_sentence_list = ner ( word_sentence_list , pos_sentence_list ) \u65b7\u8a5e\u7d50\u679c\u986f\u793a\u51fd\u5f0f \u00b6 def print_word_pos_sentence ( word_sentence , pos_sentence ): assert len ( word_sentence ) == len ( pos_sentence ) for word , pos in zip ( word_sentence , pos_sentence ): print ( f \" { word } ( { pos } )\" , end = \" \\u3000 \" ) print () return \"\"\" for i, sentence in enumerate(sentence_list): print(f\"'{sentence}'\") print_word_pos_sentence(word_sentence_list[i], pos_sentence_list[i]) for entity in sorted(entity_sentence_list[i]): print(entity) \"\"\" '\\nfor i, sentence in enumerate(sentence_list):\\n print(f\"\\'{sentence}\\'\")\\n print_word_pos_sentence(word_sentence_list[i], pos_sentence_list[i])\\n for entity in sorted(entity_sentence_list[i]):\\n print(entity)\\n' \u8a5e\u6027\u983b\u7387\u5206\u6790 \u00b6 Vlist = [] Nalist = [] for i in range ( len ( all_list )): pos_df = pd . Series ( pos_sentence_list [ i ]) . value_counts () . sort_index () . rename_axis ( 'CKIP_POS' ) . reset_index ( name = 'frequency' ) pos_cnt_V = pos_df [ pos_df . CKIP_POS != 'V_2' ] pos_cnt_V = pos_cnt_V . loc [ pos_cnt_V [ 'CKIP_POS' ] . str . contains ( 'V' )] Vlist . append ( pos_cnt_V . sum ( numeric_only = True ) . sum ()) pos_cnt_N = pos_df . loc [ pos_df [ 'CKIP_POS' ] . str . contains ( 'Na' )] Nalist . append ( pos_cnt_N . sum ( numeric_only = True ) . sum ()) all_list [ 'V_cnt' ] = Vlist all_list [ 'Na_cnt' ] = Nalist all_list [ 'V_to_Na_ratio' ] = all_list . V_cnt . div ( Nalist ) all_list . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type year sentence V_cnt Na_cnt V_to_Na_ratio 0 GSAT 105 \u9019\u6642\uff0c\u4eba\u4e5f\u53ea\u80fd\u7b11\u4e86\u3002\\n\u8607\u8fea\u52d2\u7684\u72c2\u98a8\u6a6b\u6383\u5168\u81fa\uff0c\u5b83\u7834\u58de\u4e86\uff0c\u4e5f\u5275\u9020\u4e86\u3002\u81fa\u5317\u5e02\u7684\u5169\u500b\u90f5\u7b52\u5728\u4e00\u591c\u8086\u8650... 103 72 1.430556 1 GSAT 107 \u79cb\u5929\u7e3d\u7d66\u4eba\u4e00\u7a2e\u856d\u745f\u4e4b\u611f\uff0c\u4f46\u6211\u537b\u5c0d\u79cb\u5929\u60c5\u6709\u7368\u937e\u3002\\n\u79cb\u98a8\u98af\u98af\uff0c\u6372\u8d77\u4e86\u5343\u5806\u843d\u8449\uff0c\u4e5f\u6372\u8d77\u4e86\u773e\u591a\u9077\u5ba2... 75 43 1.744186 2 AST 101 \u5b83\uff0c\u873f\u8712\u904e\u571f\u58e4\u7684\u7e2b\u9699\uff1b\u5b83\uff0c\u5bc4\u8eab\u65bc\u5927\u6d77\u7684\u6e5b\u85cd\u3002\u5b83\uff0c\u6452\u6301\u8457\u5112\u8005\u300c\u539f\u6cc9\u6efe\u6efe\uff0c\u4e0d\u542b\u665d\u591c\u3002\u300d\u7684\u7cbe\u795e\u5954\u6d41... 83 83 1.000000 3 GSAT 101 \u4eba\u751f\u662f\u4e00\u689d\u9577\u6cb3\uff0c\u552f\u6709\u5805\u786c\u7684\u5375\u77f3\u624d\u80fd\u6fc0\u76ea\u51fa\u7f8e\u9e97\u7684\u6c34\u82b1\uff0c\u4e5f\u552f\u6709\u4e00\u4efd\u9365\u800c\u4e0d\u820d\u7684\u771f\u60c5\u624d\u80fd\u62d3\u5c55\u751f\u547d\u7684\u5bec... 80 56 1.428571 4 AST 103 \u6700\u8ca7\u7aae\u7684\u4eba\u4e0d\u662f\u6c92\u6709\u9322\u8ca1\uff0c\u800c\u662f\u6c92\u6709\u5922\u60f3\u3002\u6709\u4e86\u5922\u60f3\u5c31\u50cf\u662f\u6709\u4e86\u7f85\u76e4\u7684\u822a\u884c\uff1b\u5c31\u50cf\u662f\u6709\u4e86\u5fc3\u4e2d\u7684\u7f85\u99ac\uff0c\u53ea... 99 62 1.596774 5 GSAT 98 \u73fe\u5728\u7684\u6211\u6b63\u6500\u722c\u8457\uff0c\u5728\u4e00\u7247\u770b\u4e0d\u898b\u9802\u7aef\u7684\u5c71\u58c1\u52aa\u529b\u5f80\u4e0a\u722c\uff0c\u6211\u7684\u96d9\u9830\u5145\u6eff\u4e86\u8a8d\u771f\u800c\u8f9b\u52e4\u7684\u8272\u5f69\uff0c\u4e0d\u65b7\u5730\u6709... 116 55 2.109091 6 GSAT 103 \u4eba\u751f\u5982\u5bc4\uff0c\u6b72\u6708\u5982\u68ad\u3002\u6211\u5011\u7e3d\u76fc\u671b\u65bc\u4eba\u751f\u832b\u832b\u5927\u6d77\u4e2d\uff0c\u62fe\u5f97\u5167\u5fc3\u5e95\u8655\u6700\u6e34\u671b\u7684\u6676\u7469\u73cd\u73e0\u3002\u9676\u6df5\u660e\u6240\u559c\u611b\u7684... 102 74 1.378378 7 GSAT 103 \u53e4\u4eba\u8b02\uff1a\u300c\u5929\u82e5\u6709\u60c5\uff0c\u5929\u4ea6\u8001\u300d\uff0c\u4f55\u6cc1\u70ba\u6211\u5011\u5949\u737b\u8fd1\u4e4e\u534a\u751f\u7684\u7236\u6bcd\uff1f\u7136\u800c\u8ab2\u696d\u7684\u7e41\u91cd\uff0c\u8207\u81ea\u6211\u7684\u8ff7\u832b\u53db\u9006... 122 62 1.967742 8 GSAT 98 \u6bcf\u500b\u4eba\u90fd\u6709\u5922\uff0c\u4e00\u500b\u5f9e\u5c0f\u5230\u5927\u7d30\u5fc3\u5475\u8b77\u7684\u5922\uff0c\u4e5f\u8a31\u5f88\u591a\u4eba\u89aa\u773c\u770b\u898b\u9054\u5230\u91cc\u7a0b\u7891\uff0c\u4f46\u6709\u66f4\u591a\u4eba\u7684\u5922\u6c38\u9060\u90fd\u53ea... 70 67 1.044776 9 AST 103 \u300c\u5c07\u624b\u4e2d\u7684\u71c8\u63d0\u9ad8\u4e00\u4e9b\u5427\uff01\u624d\u80fd\u7167\u4eae\u5f8c\u9762\u7684\u4eba\u3002\u300d\u6d77\u502b\u2027\u51f1\u52d2\u66fe\u6709\u6b64\u8a00\u3002\u5713\u81ea\u5df1\u4e00\u500b\u5922\u60f3\uff0c\u662f\u6211\u63d0\u8457\u624b\u4e2d... 123 103 1.194175 \u6b04\u4f4d\u4f9d\u5e8f\u8868\u793a\u70ba\uff1a\u6307\u8003\u6216\u5b78\u6e2c(type)\u3001\u5e74\u4efd(year)\u3001\u6587\u7ae0\u5167\u5bb9(sentence)\u3001\u9664V_2\u5916\u6240\u6709\u52d5\u8a5e\u6578\u91cf(V_cnt)\u3001\u666e\u901a\u540d\u8a5e\u6578\u91cf(Na_cnt)\u3001\u52d5\u8a5e\u5c0d\u540d\u8a5e\u7684\u6bd4\u4f8b(V_to_Na_ratio) vn_df = all_list [[ 'type' , 'V_cnt' , 'Na_cnt' ]] xname = 'Na_cnt' yname = 'V_cnt' vn_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type V_cnt Na_cnt 0 GSAT 103 72 1 GSAT 75 43 2 AST 83 83 3 GSAT 80 56 4 AST 99 62 ... ... ... ... 203 GSAT 119 80 204 AST 108 55 205 AST 108 66 206 AST 112 60 207 GSAT 130 84 208 rows \u00d7 3 columns \u52d5\u8a5e\u5c0d\u540d\u8a5e\u6563\u4f48\u5716 \u00b6 _ = sns . lmplot ( x = xname , y = yname , data = vn_df , ci = None , hue = 'type' ) \u7531\u4e0a\u8ff0\u7d50\u679c\u53ef\u4ee5\u767c\u73fe\u52d5\u8a5e\u666e\u904d\u8f03\u540d\u8a5e\u591a\u7684\u60c5\u6cc1\uff0c\u800c\u6307\u8003\u5728\u9019\u500b\u73fe\u8c61\u7684\u8da8\u52e2\u66f4\u70ba\u986f\u8457\u3002 \u52d5\u8a5e\u5c0d\u540d\u8a5e\u7c21\u55ae\u7dda\u6027\u56de\u6b78 \u00b6 Let \\(y_1\\) be the number of V words and \\(x_1\\) be the number of N_a words in GSAT data set. The proposed model is, \\(y_1 = \\beta_{10} + \\beta_{11} x_1 + \\epsilon_1\\) Let \\(y_2\\) be the number of V words and \\(x_2\\) be the number of N_a words in AST data set. The proposed model is, \\(y_2 = \\beta_{20} + \\beta_{21} x_2 + \\epsilon_2\\) vn_df_gsat = vn_df [ vn_df . type == 'GSAT' ] vn_df_ast = vn_df [ vn_df . type == 'AST' ] print ( \"--- GSAT ---\" ) result1 = smf . ols ( yname + '~ ' + xname , data = vn_df_gsat ) . fit () print ( result1 . summary ()) b1_1 = result1 . params [ 1 ] b0_1 = result1 . params [ 0 ] print ( f \"Estimated model: y1 = { b0_1 : .4f } + { b1_1 : .4f } x1\" ) print ( \" \\n\\n --- AST ---\" ) result2 = smf . ols ( yname + '~ ' + xname , data = vn_df_ast ) . fit () print ( result2 . summary ()) b1_2 = result2 . params [ 1 ] b0_2 = result2 . params [ 0 ] print ( f \"Estimated model: y2 = { b0_2 : .4f } + { b1_2 : .4f } x2\" ) --- GSAT --- OLS Regression Results ============================================================================== Dep. Variable: V_cnt R-squared: 0.068 Model: OLS Adj. R-squared: 0.060 Method: Least Squares F-statistic: 8.212 Date: Thu, 17 Jun 2021 Prob (F-statistic): 0.00496 Time: 13:05:05 Log-Likelihood: -488.55 No. Observations: 115 AIC: 981.1 Df Residuals: 113 BIC: 986.6 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 84.8264 7.885 10.758 0.000 69.205 100.448 Na_cnt 0.3055 0.107 2.866 0.005 0.094 0.517 ============================================================================== Omnibus: 1.252 Durbin-Watson: 2.118 Prob(Omnibus): 0.535 Jarque-Bera (JB): 1.226 Skew: -0.135 Prob(JB): 0.542 Kurtosis: 2.573 Cond. No. 366. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Estimated model: y1 = 84.8264 + 0.3055 x1 --- AST --- OLS Regression Results ============================================================================== Dep. Variable: V_cnt R-squared: 0.505 Model: OLS Adj. R-squared: 0.500 Method: Least Squares F-statistic: 92.93 Date: Thu, 17 Jun 2021 Prob (F-statistic): 1.45e-15 Time: 13:05:05 Log-Likelihood: -375.58 No. Observations: 93 AIC: 755.2 Df Residuals: 91 BIC: 760.2 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 46.2938 6.124 7.559 0.000 34.129 58.459 Na_cnt 0.8510 0.088 9.640 0.000 0.676 1.026 ============================================================================== Omnibus: 5.029 Durbin-Watson: 1.645 Prob(Omnibus): 0.081 Jarque-Bera (JB): 5.690 Skew: 0.250 Prob(JB): 0.0581 Kurtosis: 4.104 Cond. No. 295. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Estimated model: y2 = 46.2938 + 0.8510 x2 \u5c0d\u6307\u8003\u8207\u5b78\u6e2c\u7684\u52d5\u8a5e\u8207\u540d\u8a5e\u5206\u4f48\u5206\u5225\u9032\u884c\u7c21\u55ae\u7dda\u6027\u8ff4\u6b78\u5206\u6790\uff1a \u53ef\u4ee5\u767c\u73fe\u5b78\u6e2c\u8cc7\u6599\u7684 \\(R^2=0.068\\) \u986f\u793a\u8a72\u6a21\u578b\u89e3\u91cb\u529b\u4e0d\u8db3\u3002 \u5169\u8cc7\u6599\u7684\u6a21\u578bF\u6aa2\u5b9a\u8207\u53c3\u6578t\u6aa2\u5b9a\u7686\u70ba\u986f\u8457\u3002 \u6b98\u503c\u5206\u6790\u7b49\u66ab\u4e14\u5ffd\u7565\u3002 \u52d5\u8a5e\u5c0d\u540d\u8a5e\u6bd4\u4f8bHistogram \u00b6 plot = all_list . hist ( column = 'V_to_Na_ratio' ) print ( \"ratio mean = \" , all_list . V_to_Na_ratio . mean ()) ratio mean = 1.5512316568733755 \u756b\u51faHistogram\u5f8c\u53ef\u4ee5\u767c\u73fe\u6bd4\u4f8b\u7684\u5e73\u5747\u503c\u70ba1.551\u5de6\u53f3\uff0c\u773e\u6578\u4e5f\u4f4d\u5728\u76f8\u8fd1\u7684\u4f4d\u7f6e\uff0c\u6574\u9ad4\u5206\u4f48\u5448\u73febell-shape\u3002 \u55ae\u8a5e\u8a5e\u610f\u8a08\u7b97\u51fd\u5f0f \u00b6 cwn = CwnBase () def all_sense_tree ( word , verbal = False ): cnt = 0 for i in range ( len ( word )): snese_tree = word [ i ] . senses cnt += len ( word [ i ] . senses ) if ( verbal == True ): print ( snese_tree ) if ( verbal == True ): print ( \"total senses = \" , cnt ) return cnt ''' _word = word_sentence_list[108][54] word = cwn.find_lemma(\"^\" + _word + \"$\") print(\"word: \", _word) all_sense_tree(word, verbal = True) ''' '\\n_word = word_sentence_list[108][54]\\nword = cwn.find_lemma(\"^\" + _word + \"$\")\\nprint(\"word: \", _word)\\nall_sense_tree(word, verbal = True)\\n' \u55ae\u8a5e\u8a5e\u610f\u91cf\u8a08\u7b97 \u00b6 pun_set = { \"COLONCATEGORY\" , \"COMMACATEGORY\" , \"DASHCATEGORY\" , \"ETCCATEGORY\" , \"EXCLAMATIONCATEGORY\" , \"PARENTHESISCATEGORY\" , \"PAUSECATEGORY\" , \"PERIODCATEGORY\" , \"QUESTIONCATEGORY\" , \"SEMICOLONCATEGORY\" , \"SPCHANGECATEGORY\" } all_senses_list = list () all_senses_list_sum = list () for i in range ( all_list . shape [ 0 ]): senses_list = list () arr = pd . Series ( word_sentence_list [ i ]) ttl = 0 for j in range ( len ( arr )): if ( pos_sentence_list [ i ][ j ] not in pun_set ): _word = arr [ j ] word = cwn . find_lemma ( \"^\" + _word + \"$\" ) sense_cnt = all_sense_tree ( word ) senses_list . append (( arr [ j ], pos_sentence_list [ i ][ j ], sense_cnt )) tp = sentence_list_type [ i ] year = sentence_list_year [ i ] all_senses_list . append (( tp , year , senses_list )) tmp_df = pd . DataFrame ( senses_list , columns = [ 'tagged_word' , 'CKIP_POS' , 'sense_cnt' ]) words_cnt = tmp_df . shape [ 0 ] tmp_df_ = tmp_df [ tmp_df [ 'sense_cnt' ] != 0 ] words_cnt_ = tmp_df_ . shape [ 0 ] all_senses_list_sum . append (( tp , year , words_cnt , tmp_df . sense_cnt . mean (), words_cnt_ , tmp_df_ . sense_cnt . mean ())) all_senses_df = pd . DataFrame ( all_senses_list , columns = [ 'type' , 'year' , 'tagged_words' ]) all_senses_df_sum = pd . DataFrame ( all_senses_list_sum , columns = [ 'type' , 'year' , 'words_cnt' , 'avg_sense_all' , 'words_cnt_nonzero' , 'avg_sense_nonzero' ]) all_senses_df_sum [ 'zero_sense_ratio' ] = all_senses_df_sum . words_cnt_nonzero . div ( words_cnt ) all_senses_df_sum = all_senses_df_sum . assign ( zero_sense_ratio = 1 - all_senses_df_sum [ 'zero_sense_ratio' ]) display ( all_senses_df_sum . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type year words_cnt avg_sense_all words_cnt_nonzero avg_sense_nonzero zero_sense_ratio 0 GSAT 105 399 5.100251 300 6.783333 0.353448 1 GSAT 107 268 5.313433 193 7.378238 0.584052 2 AST 101 345 6.191304 242 8.826446 0.478448 3 GSAT 101 345 6.011594 264 7.856061 0.431034 4 AST 103 394 5.581218 305 7.209836 0.342672 \u5e73\u5747\u8a5e\u610f\u91cfHistogram \u00b6 print ( \"average sense from all words = \" , all_senses_df_sum . avg_sense_all . mean ()) print ( \"average sense from non-zero-sensed words = \" , all_senses_df_sum . avg_sense_nonzero . mean ()) bins = np . linspace ( 4 , 11 , 35 ) plt . hist ( all_senses_df_sum . avg_sense_all , bins , alpha = 0.7 , label = 'avg sense - all words' ) plt . hist ( all_senses_df_sum . avg_sense_nonzero , bins , alpha = 0.7 , label = 'avg sense - non-zero-sensed words' ) plt . legend ( loc = 'upper right' , bbox_to_anchor = ( 1.7 , 1 )) plt . grid ( True ) plt . show () average sense from all words = 5.9117085939980205 average sense from non-zero-sensed words = 8.013096983316954 \u7d93\u904e\u8a08\u7b97\u5f8c\u53ef\u4ee5\u5f97\u77e5\u5728\u6240\u6709\u8cc7\u6599\u4e2d\uff0c\u5e73\u5747\u8fad\u610f\u70ba5.91\uff1b\u6263\u9664\u8fad\u610f\u70ba0\u7684\u55ae\u8a5e\u5f8c\u5e73\u5747\u8fad\u610f\u70ba8.01\u3002 \u51b7\u50fb\u8a5e\u4f7f\u7528\u5206\u6790 \u00b6 \u82e5\u6211\u5011\u5047\u8a2d\u8a5e\u610f\u91cf\u70ba\u96f6\u4e4b\u55ae\u8a5e\u662f\u51b7\u50fb\u8a5e\uff0c\u7d93\u904e\u8a08\u7b97\u53ef\u4ee5\u5f97\u5230\u6bcf\u7bc7\u6587\u7ae0\u4f7f\u7528\u51b7\u50fb\u8a5e\u7684\u6bd4\u7387\uff1a plot = all_senses_df_sum . hist ( column = 'zero_sense_ratio' ) print ( \"ratio mean = \" , all_senses_df_sum . zero_sense_ratio . mean ()) ratio mean = 0.3614161969496021 \u53ef\u898b\u5728\u6240\u6709\u8cc7\u6599\u96c6\u4e2d\uff0c\u51b7\u50fb\u5b57\u7684\u4f7f\u7528\u5e73\u5747\u6bd4\u4f8b\u70ba36.14%\u3002 \u7279\u5b9a\u8a5e\u6027\u55ae\u8a5e\u51fa\u73fe\u983b\u7387 \u00b6 tmp_list = list () for i in range ( all_senses_df . shape [ 0 ]): tmp_list += all_senses_df . tagged_words [ i ] senses_df = pd . DataFrame ( tmp_list , columns = [ 'tagged_word' , 'CKIP_POS' , 'sense_cnt' ]) senses_df_ = senses_df [ senses_df [ 'CKIP_POS' ] != 'WHITESPACE' ] #delete \\n senses_df_ = senses_df_ [ senses_df_ [ 'tagged_word' ] != '\u3002 \\n ' ] #delete \u3002\\n #senses_df_.head() words_cnt_ = pd . DataFrame ( senses_df_ . value_counts ()) print ( \"Head of frequency of all words\" ) display ( words_cnt_ . head ( 10 )) words_cnt_Na = senses_df_ [ senses_df_ . CKIP_POS == 'Na' ] words_cnt_Na = pd . DataFrame ( words_cnt_Na . value_counts ()) print ( \" \\n\\n Head of frequency of Na(\u666e\u901a\u540d\u8a5e) words\" ) display ( words_cnt_Na . head ( 10 )) words_cnt_Nb = senses_df_ [ senses_df_ . CKIP_POS == 'Nb' ] words_cnt_Nb = pd . DataFrame ( words_cnt_Nb . value_counts ()) print ( \" \\n\\n Head of frequency of Nb(\u5c08\u6709\u540d\u7a31) words\" ) display ( words_cnt_Nb . head ( 10 )) words_cnt_V = senses_df_ [ senses_df_ . CKIP_POS != 'V_2' ] words_cnt_V = words_cnt_V . loc [ words_cnt_V [ 'CKIP_POS' ] . str . contains ( 'V' )] words_cnt_V = pd . DataFrame ( words_cnt_V . value_counts ()) print ( \" \\n\\n Head of frequency of V words (excluding V_2)\" ) display ( words_cnt_V . head ( 10 )) words_cnt_VA = senses_df_ [ senses_df_ . CKIP_POS == 'VA' ] words_cnt_VA = pd . DataFrame ( words_cnt_VA . value_counts ()) print ( \" \\n\\n Head of frequency of VA(\u52d5\u4f5c\u4e0d\u53ca\u7269\u52d5\u8a5e) words\" ) display ( words_cnt_VA . head ( 10 )) Head of frequency of all words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u7684 DE 16 7052 \u6211 Nh 3 2705 \u4e00 Neu 10 1454 \u662f SHI 9 1362 \u5728 P 10 1229 \u4e86 Di 5 836 \u4e0d D 3 699 \u8457 Di 8 652 \u81ea\u5df1 Nh 5 601 \u800c Cbb 10 569 Head of frequency of Na(\u666e\u901a\u540d\u8a5e) words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u4eba Na 11 481 \u5fc3 Na 13 247 \u751f\u547d Na 8 234 \u4eba\u751f Na 1 206 \u5922\u60f3 Na 0 107 \u5b78\u751f Na 2 86 \u9006\u5883 Na 0 83 \u5922 Na 3 81 \u90f5\u7b52 Na 0 78 \u751f\u6d3b Na 3 78 Head of frequency of Nb(\u5c08\u6709\u540d\u7a31) words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u5b54\u5b50 Nb 3 13 \u9805\u7fbd Nb 0 11 \u674e\u767d Nb 2 11 \u8607\u8fea\u52d2 Nb 0 10 \u8607\u8efe Nb 0 8 \u67f3\u5b97\u5143 Nb 0 8 \u67ef\u9ea5\u9686 Nb 0 8 \u9676\u6df5\u660e Nb 0 7 \u674e\u5b89 Nb 0 7 \u53f2\u8a18 Nb 1 6 Head of frequency of V words (excluding V_2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u8b93 VL 6 183 \u4f7f VL 13 181 \u8aaa VE 16 132 \u5927 VH 28 120 \u9762\u5c0d VC 2 119 \u6c92\u6709 VJ 6 107 \u60f3 VE 9 106 \u6df1 VH 17 88 \u770b VC 13 88 \u6109\u5feb VH 1 87 Head of frequency of VA(\u52d5\u4f5c\u4e0d\u53ca\u7269\u52d5\u8a5e) words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u61c9\u8b8a VA 0 45 \u4f86 VA 19 44 \u901a\u95dc VA 0 39 \u5ea6\u4eba VA 0 36 \u5b58\u5728 VA 3 32 \u7b11 VA 2 30 \u6b6a\u8170 VA 0 29 \u7ad9 VA 7 23 \u8d70 VA 20 21 \u5750 VA 12 19 \u4ee5\u4e0a\u6211\u5011\u7d71\u8a08\u4e86\u5e7e\u500b\u7279\u5b9a\u8a5e\u6027\u5728\u6240\u6709\u8cc7\u6599\u96c6\u7684\u55ae\u8a5e\u51fa\u73fe\u983b\u7387\u3002 \u9023\u7d50 \u00b6 \u56de\u5230 R Project \u7814\u7a76\u7d50\u679c\u9801\u9762 \u56de\u5230 \u5165\u53e3\u9801\u9762","title":"\u53f0\u7063\u5347\u5927\u5b78\u8003\u8a66\u570b\u5beb\u60c5\u610f\u984c\u4f73\u4f5c\u8a5e\u983b\u76f8\u95dc\u7814\u7a76"},{"location":"#_1","text":"\u7b2c\u4e09\u7d44 \u60f3\u4e0d\u5230\u7d44\u540d \u7d44\u54e1\uff1a\u76e7\u5fb7\u539f\u3001\u6731\u4fee\u5e73\u3001\u694a\u8212\u6674\u3001\u9673\u5b9b\u7469","title":"\u53f0\u7063\u5347\u5927\u5b78\u8003\u8a66\u570b\u5beb\u60c5\u610f\u984c\u4f73\u4f5c\u8a5e\u983b\u76f8\u95dc\u7814\u7a76"},{"location":"#python-project","text":"import numpy as np import pandas as pd import gdown import os import re import seaborn as sns from matplotlib import pyplot as plt import statsmodels.formula.api as smf import torch import transformers import CwnSenseTagger #CwnSenseTagger.download() import CwnGraph #CwnGraph.download() from CwnGraph import CwnBase from ckiptagger import data_utils , construct_dictionary , WS , POS , NER #import DistilTag #DistilTag.download() ws = WS ( os . path . abspath ( os . path . join ( os . getcwd (), os . path . pardir )) + '/data' ) pos = POS ( os . path . abspath ( os . path . join ( os . getcwd (), os . path . pardir )) + '/data' ) ner = NER ( os . path . abspath ( os . path . join ( os . getcwd (), os . path . pardir )) + '/data' ) /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0. warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \" /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead. warnings.warn('`layer.add_variable` is deprecated and '","title":"Python Project"},{"location":"#_2","text":"all_f = [] for file in os . listdir ( \"data_set/\" ): if file . endswith ( \".txt\" ): all_f . append ( file ) ''' print(\"list length = \", len(all_f)) print(all_f[2]) print(len(all_f[2])) ''' '\\nprint(\"list length = \", len(all_f))\\nprint(all_f[2])\\nprint(len(all_f[2]))\\n' sentence_list = [] sentence_list_type = [] sentence_list_year = [] for i in all_f : f = open ( \"data_set/\" + i ) sentence_list . append ( f . read ()) if \"GSAT\" in i : sentence_list_type . append ( \"GSAT\" ) else : sentence_list_type . append ( \"AST\" ) year = re . search ( \"\\_(...|..)\\_\" , i ) . group ( 1 ) sentence_list_year . append ( year ) all_list = pd . DataFrame ({ 'type' : sentence_list_type , 'year' : sentence_list_year , 'sentence' : sentence_list }) all_list .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type year sentence 0 GSAT 105 \u9019\u6642\uff0c\u4eba\u4e5f\u53ea\u80fd\u7b11\u4e86\u3002\\n\u8607\u8fea\u52d2\u7684\u72c2\u98a8\u6a6b\u6383\u5168\u81fa\uff0c\u5b83\u7834\u58de\u4e86\uff0c\u4e5f\u5275\u9020\u4e86\u3002\u81fa\u5317\u5e02\u7684\u5169\u500b\u90f5\u7b52\u5728\u4e00\u591c\u8086\u8650... 1 GSAT 107 \u79cb\u5929\u7e3d\u7d66\u4eba\u4e00\u7a2e\u856d\u745f\u4e4b\u611f\uff0c\u4f46\u6211\u537b\u5c0d\u79cb\u5929\u60c5\u6709\u7368\u937e\u3002\\n\u79cb\u98a8\u98af\u98af\uff0c\u6372\u8d77\u4e86\u5343\u5806\u843d\u8449\uff0c\u4e5f\u6372\u8d77\u4e86\u773e\u591a\u9077\u5ba2... 2 AST 101 \u5b83\uff0c\u873f\u8712\u904e\u571f\u58e4\u7684\u7e2b\u9699\uff1b\u5b83\uff0c\u5bc4\u8eab\u65bc\u5927\u6d77\u7684\u6e5b\u85cd\u3002\u5b83\uff0c\u6452\u6301\u8457\u5112\u8005\u300c\u539f\u6cc9\u6efe\u6efe\uff0c\u4e0d\u542b\u665d\u591c\u3002\u300d\u7684\u7cbe\u795e\u5954\u6d41... 3 GSAT 101 \u4eba\u751f\u662f\u4e00\u689d\u9577\u6cb3\uff0c\u552f\u6709\u5805\u786c\u7684\u5375\u77f3\u624d\u80fd\u6fc0\u76ea\u51fa\u7f8e\u9e97\u7684\u6c34\u82b1\uff0c\u4e5f\u552f\u6709\u4e00\u4efd\u9365\u800c\u4e0d\u820d\u7684\u771f\u60c5\u624d\u80fd\u62d3\u5c55\u751f\u547d\u7684\u5bec... 4 AST 103 \u6700\u8ca7\u7aae\u7684\u4eba\u4e0d\u662f\u6c92\u6709\u9322\u8ca1\uff0c\u800c\u662f\u6c92\u6709\u5922\u60f3\u3002\u6709\u4e86\u5922\u60f3\u5c31\u50cf\u662f\u6709\u4e86\u7f85\u76e4\u7684\u822a\u884c\uff1b\u5c31\u50cf\u662f\u6709\u4e86\u5fc3\u4e2d\u7684\u7f85\u99ac\uff0c\u53ea... ... ... ... ... 203 GSAT 110 \u5982\u679c\u6211\u6709\u4e00\u5ea7\u65b0\u51b0\u7bb1\uff0c\u90a3\u60f3\u5fc5\u662f\u6e05\u65b0\u53ef\u4eba\uff0c\u689d\u7406\u5206\u660e\u3002\u56e0\u70ba\u51b0\u85cf\u5728\u5176\u9593\u7684\uff0c\u4e0d\u50c5\u662f\u98df\u7269\u672c\u8eab\uff0c\u66f4\u860a\u542b\u80cc\u5f8c... 204 AST 101 \u4eba\u7e3d\u514d\u4e0d\u4e86\u81ea\u5df1\u4e00\u500b\u4eba\u7684\u3002\u6709\u4eba\u6015\u5bc2\u5bde\uff0c\u8aaa\uff1a\u300c\u5bc2\u5bde\uff0c\u96e3\u8010\u3002\u300d\u4ed6\u5011\u6015\u5b64\u7368\uff0c\u5b64\u7368\u8b93\u4ed6\u5011\u60f6\u6050\u3001\u5bb3\u6015\uff0c\u611f... 205 AST 103 \u6211\u7684\u7956\u7236\u5df2\u7d93\u9ad8\u9f61\u4e5d\u5341\u4e8c\u6b72\uff0c\u4ed6\u8eab\u908a\u7684\u4eba\u6b63\u4e00\u500b\u500b\u96e2\u4ed6\u800c\u53bb\u3002\u6709\u4e00\u6b21\u548c\u6211\u804a\u5929\u7684\u6642\u5019\uff0c\u6211\u5f9e\u4ed6\u53e3\u4e2d\u59d4\u5a49\u7684... 206 AST 105 \u300c\u9806\u98a8\u53ef\u4ee5\u822a\u884c\uff0c\u9006\u98a8\u53ef\u4ee5\u98db\u884c\u3002\u300d\u751f\u547d\uff0c\u662f\u4e00\u9023\u4e32\u7684\u529f\u8ab2\uff0c\u5176\u4e2d\u4e4b\u4e00\uff0c\u4fbf\u662f\u300c\u8209\u91cd\u82e5\u8f15\u300d\u7684\u5b78\u554f\u3002\u9806\u5883... 207 GSAT 102 \u97f3\u6a02\uff0c\u98c4\u63da\u65bc\u7a7a\u4e2d\uff0c\u90a3\u662f\u807d\u89ba\u7684\u6109\u5feb\uff1b\u9178\u751c\u82e6\u8fa3\uff0c\u649e\u64ca\u5473\u857e\uff0c\u90a3\u662f\u5473\u89ba\u7684\u6109\u5feb\uff1b\u300c\u6c99\u9dd7\u7fd4\u96c6\uff0c\u9326\u9c57\u6e38\u6cf3\u300d... 208 rows \u00d7 3 columns","title":"\u8b80\u53d6\u8207\u6574\u7406\u8cc7\u6599\u96c6"},{"location":"#ckiptagger","text":"word_sentence_list = ws ( sentence_list ) pos_sentence_list = pos ( word_sentence_list ) entity_sentence_list = ner ( word_sentence_list , pos_sentence_list )","title":"ckiptagger"},{"location":"#_3","text":"def print_word_pos_sentence ( word_sentence , pos_sentence ): assert len ( word_sentence ) == len ( pos_sentence ) for word , pos in zip ( word_sentence , pos_sentence ): print ( f \" { word } ( { pos } )\" , end = \" \\u3000 \" ) print () return \"\"\" for i, sentence in enumerate(sentence_list): print(f\"'{sentence}'\") print_word_pos_sentence(word_sentence_list[i], pos_sentence_list[i]) for entity in sorted(entity_sentence_list[i]): print(entity) \"\"\" '\\nfor i, sentence in enumerate(sentence_list):\\n print(f\"\\'{sentence}\\'\")\\n print_word_pos_sentence(word_sentence_list[i], pos_sentence_list[i])\\n for entity in sorted(entity_sentence_list[i]):\\n print(entity)\\n'","title":"\u65b7\u8a5e\u7d50\u679c\u986f\u793a\u51fd\u5f0f"},{"location":"#_4","text":"Vlist = [] Nalist = [] for i in range ( len ( all_list )): pos_df = pd . Series ( pos_sentence_list [ i ]) . value_counts () . sort_index () . rename_axis ( 'CKIP_POS' ) . reset_index ( name = 'frequency' ) pos_cnt_V = pos_df [ pos_df . CKIP_POS != 'V_2' ] pos_cnt_V = pos_cnt_V . loc [ pos_cnt_V [ 'CKIP_POS' ] . str . contains ( 'V' )] Vlist . append ( pos_cnt_V . sum ( numeric_only = True ) . sum ()) pos_cnt_N = pos_df . loc [ pos_df [ 'CKIP_POS' ] . str . contains ( 'Na' )] Nalist . append ( pos_cnt_N . sum ( numeric_only = True ) . sum ()) all_list [ 'V_cnt' ] = Vlist all_list [ 'Na_cnt' ] = Nalist all_list [ 'V_to_Na_ratio' ] = all_list . V_cnt . div ( Nalist ) all_list . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type year sentence V_cnt Na_cnt V_to_Na_ratio 0 GSAT 105 \u9019\u6642\uff0c\u4eba\u4e5f\u53ea\u80fd\u7b11\u4e86\u3002\\n\u8607\u8fea\u52d2\u7684\u72c2\u98a8\u6a6b\u6383\u5168\u81fa\uff0c\u5b83\u7834\u58de\u4e86\uff0c\u4e5f\u5275\u9020\u4e86\u3002\u81fa\u5317\u5e02\u7684\u5169\u500b\u90f5\u7b52\u5728\u4e00\u591c\u8086\u8650... 103 72 1.430556 1 GSAT 107 \u79cb\u5929\u7e3d\u7d66\u4eba\u4e00\u7a2e\u856d\u745f\u4e4b\u611f\uff0c\u4f46\u6211\u537b\u5c0d\u79cb\u5929\u60c5\u6709\u7368\u937e\u3002\\n\u79cb\u98a8\u98af\u98af\uff0c\u6372\u8d77\u4e86\u5343\u5806\u843d\u8449\uff0c\u4e5f\u6372\u8d77\u4e86\u773e\u591a\u9077\u5ba2... 75 43 1.744186 2 AST 101 \u5b83\uff0c\u873f\u8712\u904e\u571f\u58e4\u7684\u7e2b\u9699\uff1b\u5b83\uff0c\u5bc4\u8eab\u65bc\u5927\u6d77\u7684\u6e5b\u85cd\u3002\u5b83\uff0c\u6452\u6301\u8457\u5112\u8005\u300c\u539f\u6cc9\u6efe\u6efe\uff0c\u4e0d\u542b\u665d\u591c\u3002\u300d\u7684\u7cbe\u795e\u5954\u6d41... 83 83 1.000000 3 GSAT 101 \u4eba\u751f\u662f\u4e00\u689d\u9577\u6cb3\uff0c\u552f\u6709\u5805\u786c\u7684\u5375\u77f3\u624d\u80fd\u6fc0\u76ea\u51fa\u7f8e\u9e97\u7684\u6c34\u82b1\uff0c\u4e5f\u552f\u6709\u4e00\u4efd\u9365\u800c\u4e0d\u820d\u7684\u771f\u60c5\u624d\u80fd\u62d3\u5c55\u751f\u547d\u7684\u5bec... 80 56 1.428571 4 AST 103 \u6700\u8ca7\u7aae\u7684\u4eba\u4e0d\u662f\u6c92\u6709\u9322\u8ca1\uff0c\u800c\u662f\u6c92\u6709\u5922\u60f3\u3002\u6709\u4e86\u5922\u60f3\u5c31\u50cf\u662f\u6709\u4e86\u7f85\u76e4\u7684\u822a\u884c\uff1b\u5c31\u50cf\u662f\u6709\u4e86\u5fc3\u4e2d\u7684\u7f85\u99ac\uff0c\u53ea... 99 62 1.596774 5 GSAT 98 \u73fe\u5728\u7684\u6211\u6b63\u6500\u722c\u8457\uff0c\u5728\u4e00\u7247\u770b\u4e0d\u898b\u9802\u7aef\u7684\u5c71\u58c1\u52aa\u529b\u5f80\u4e0a\u722c\uff0c\u6211\u7684\u96d9\u9830\u5145\u6eff\u4e86\u8a8d\u771f\u800c\u8f9b\u52e4\u7684\u8272\u5f69\uff0c\u4e0d\u65b7\u5730\u6709... 116 55 2.109091 6 GSAT 103 \u4eba\u751f\u5982\u5bc4\uff0c\u6b72\u6708\u5982\u68ad\u3002\u6211\u5011\u7e3d\u76fc\u671b\u65bc\u4eba\u751f\u832b\u832b\u5927\u6d77\u4e2d\uff0c\u62fe\u5f97\u5167\u5fc3\u5e95\u8655\u6700\u6e34\u671b\u7684\u6676\u7469\u73cd\u73e0\u3002\u9676\u6df5\u660e\u6240\u559c\u611b\u7684... 102 74 1.378378 7 GSAT 103 \u53e4\u4eba\u8b02\uff1a\u300c\u5929\u82e5\u6709\u60c5\uff0c\u5929\u4ea6\u8001\u300d\uff0c\u4f55\u6cc1\u70ba\u6211\u5011\u5949\u737b\u8fd1\u4e4e\u534a\u751f\u7684\u7236\u6bcd\uff1f\u7136\u800c\u8ab2\u696d\u7684\u7e41\u91cd\uff0c\u8207\u81ea\u6211\u7684\u8ff7\u832b\u53db\u9006... 122 62 1.967742 8 GSAT 98 \u6bcf\u500b\u4eba\u90fd\u6709\u5922\uff0c\u4e00\u500b\u5f9e\u5c0f\u5230\u5927\u7d30\u5fc3\u5475\u8b77\u7684\u5922\uff0c\u4e5f\u8a31\u5f88\u591a\u4eba\u89aa\u773c\u770b\u898b\u9054\u5230\u91cc\u7a0b\u7891\uff0c\u4f46\u6709\u66f4\u591a\u4eba\u7684\u5922\u6c38\u9060\u90fd\u53ea... 70 67 1.044776 9 AST 103 \u300c\u5c07\u624b\u4e2d\u7684\u71c8\u63d0\u9ad8\u4e00\u4e9b\u5427\uff01\u624d\u80fd\u7167\u4eae\u5f8c\u9762\u7684\u4eba\u3002\u300d\u6d77\u502b\u2027\u51f1\u52d2\u66fe\u6709\u6b64\u8a00\u3002\u5713\u81ea\u5df1\u4e00\u500b\u5922\u60f3\uff0c\u662f\u6211\u63d0\u8457\u624b\u4e2d... 123 103 1.194175 \u6b04\u4f4d\u4f9d\u5e8f\u8868\u793a\u70ba\uff1a\u6307\u8003\u6216\u5b78\u6e2c(type)\u3001\u5e74\u4efd(year)\u3001\u6587\u7ae0\u5167\u5bb9(sentence)\u3001\u9664V_2\u5916\u6240\u6709\u52d5\u8a5e\u6578\u91cf(V_cnt)\u3001\u666e\u901a\u540d\u8a5e\u6578\u91cf(Na_cnt)\u3001\u52d5\u8a5e\u5c0d\u540d\u8a5e\u7684\u6bd4\u4f8b(V_to_Na_ratio) vn_df = all_list [[ 'type' , 'V_cnt' , 'Na_cnt' ]] xname = 'Na_cnt' yname = 'V_cnt' vn_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type V_cnt Na_cnt 0 GSAT 103 72 1 GSAT 75 43 2 AST 83 83 3 GSAT 80 56 4 AST 99 62 ... ... ... ... 203 GSAT 119 80 204 AST 108 55 205 AST 108 66 206 AST 112 60 207 GSAT 130 84 208 rows \u00d7 3 columns","title":"\u8a5e\u6027\u983b\u7387\u5206\u6790"},{"location":"#_5","text":"_ = sns . lmplot ( x = xname , y = yname , data = vn_df , ci = None , hue = 'type' ) \u7531\u4e0a\u8ff0\u7d50\u679c\u53ef\u4ee5\u767c\u73fe\u52d5\u8a5e\u666e\u904d\u8f03\u540d\u8a5e\u591a\u7684\u60c5\u6cc1\uff0c\u800c\u6307\u8003\u5728\u9019\u500b\u73fe\u8c61\u7684\u8da8\u52e2\u66f4\u70ba\u986f\u8457\u3002","title":"\u52d5\u8a5e\u5c0d\u540d\u8a5e\u6563\u4f48\u5716"},{"location":"#_6","text":"Let \\(y_1\\) be the number of V words and \\(x_1\\) be the number of N_a words in GSAT data set. The proposed model is, \\(y_1 = \\beta_{10} + \\beta_{11} x_1 + \\epsilon_1\\) Let \\(y_2\\) be the number of V words and \\(x_2\\) be the number of N_a words in AST data set. The proposed model is, \\(y_2 = \\beta_{20} + \\beta_{21} x_2 + \\epsilon_2\\) vn_df_gsat = vn_df [ vn_df . type == 'GSAT' ] vn_df_ast = vn_df [ vn_df . type == 'AST' ] print ( \"--- GSAT ---\" ) result1 = smf . ols ( yname + '~ ' + xname , data = vn_df_gsat ) . fit () print ( result1 . summary ()) b1_1 = result1 . params [ 1 ] b0_1 = result1 . params [ 0 ] print ( f \"Estimated model: y1 = { b0_1 : .4f } + { b1_1 : .4f } x1\" ) print ( \" \\n\\n --- AST ---\" ) result2 = smf . ols ( yname + '~ ' + xname , data = vn_df_ast ) . fit () print ( result2 . summary ()) b1_2 = result2 . params [ 1 ] b0_2 = result2 . params [ 0 ] print ( f \"Estimated model: y2 = { b0_2 : .4f } + { b1_2 : .4f } x2\" ) --- GSAT --- OLS Regression Results ============================================================================== Dep. Variable: V_cnt R-squared: 0.068 Model: OLS Adj. R-squared: 0.060 Method: Least Squares F-statistic: 8.212 Date: Thu, 17 Jun 2021 Prob (F-statistic): 0.00496 Time: 13:05:05 Log-Likelihood: -488.55 No. Observations: 115 AIC: 981.1 Df Residuals: 113 BIC: 986.6 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 84.8264 7.885 10.758 0.000 69.205 100.448 Na_cnt 0.3055 0.107 2.866 0.005 0.094 0.517 ============================================================================== Omnibus: 1.252 Durbin-Watson: 2.118 Prob(Omnibus): 0.535 Jarque-Bera (JB): 1.226 Skew: -0.135 Prob(JB): 0.542 Kurtosis: 2.573 Cond. No. 366. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Estimated model: y1 = 84.8264 + 0.3055 x1 --- AST --- OLS Regression Results ============================================================================== Dep. Variable: V_cnt R-squared: 0.505 Model: OLS Adj. R-squared: 0.500 Method: Least Squares F-statistic: 92.93 Date: Thu, 17 Jun 2021 Prob (F-statistic): 1.45e-15 Time: 13:05:05 Log-Likelihood: -375.58 No. Observations: 93 AIC: 755.2 Df Residuals: 91 BIC: 760.2 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept 46.2938 6.124 7.559 0.000 34.129 58.459 Na_cnt 0.8510 0.088 9.640 0.000 0.676 1.026 ============================================================================== Omnibus: 5.029 Durbin-Watson: 1.645 Prob(Omnibus): 0.081 Jarque-Bera (JB): 5.690 Skew: 0.250 Prob(JB): 0.0581 Kurtosis: 4.104 Cond. No. 295. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Estimated model: y2 = 46.2938 + 0.8510 x2 \u5c0d\u6307\u8003\u8207\u5b78\u6e2c\u7684\u52d5\u8a5e\u8207\u540d\u8a5e\u5206\u4f48\u5206\u5225\u9032\u884c\u7c21\u55ae\u7dda\u6027\u8ff4\u6b78\u5206\u6790\uff1a \u53ef\u4ee5\u767c\u73fe\u5b78\u6e2c\u8cc7\u6599\u7684 \\(R^2=0.068\\) \u986f\u793a\u8a72\u6a21\u578b\u89e3\u91cb\u529b\u4e0d\u8db3\u3002 \u5169\u8cc7\u6599\u7684\u6a21\u578bF\u6aa2\u5b9a\u8207\u53c3\u6578t\u6aa2\u5b9a\u7686\u70ba\u986f\u8457\u3002 \u6b98\u503c\u5206\u6790\u7b49\u66ab\u4e14\u5ffd\u7565\u3002","title":"\u52d5\u8a5e\u5c0d\u540d\u8a5e\u7c21\u55ae\u7dda\u6027\u56de\u6b78"},{"location":"#histogram","text":"plot = all_list . hist ( column = 'V_to_Na_ratio' ) print ( \"ratio mean = \" , all_list . V_to_Na_ratio . mean ()) ratio mean = 1.5512316568733755 \u756b\u51faHistogram\u5f8c\u53ef\u4ee5\u767c\u73fe\u6bd4\u4f8b\u7684\u5e73\u5747\u503c\u70ba1.551\u5de6\u53f3\uff0c\u773e\u6578\u4e5f\u4f4d\u5728\u76f8\u8fd1\u7684\u4f4d\u7f6e\uff0c\u6574\u9ad4\u5206\u4f48\u5448\u73febell-shape\u3002","title":"\u52d5\u8a5e\u5c0d\u540d\u8a5e\u6bd4\u4f8bHistogram"},{"location":"#_7","text":"cwn = CwnBase () def all_sense_tree ( word , verbal = False ): cnt = 0 for i in range ( len ( word )): snese_tree = word [ i ] . senses cnt += len ( word [ i ] . senses ) if ( verbal == True ): print ( snese_tree ) if ( verbal == True ): print ( \"total senses = \" , cnt ) return cnt ''' _word = word_sentence_list[108][54] word = cwn.find_lemma(\"^\" + _word + \"$\") print(\"word: \", _word) all_sense_tree(word, verbal = True) ''' '\\n_word = word_sentence_list[108][54]\\nword = cwn.find_lemma(\"^\" + _word + \"$\")\\nprint(\"word: \", _word)\\nall_sense_tree(word, verbal = True)\\n'","title":"\u55ae\u8a5e\u8a5e\u610f\u8a08\u7b97\u51fd\u5f0f"},{"location":"#_8","text":"pun_set = { \"COLONCATEGORY\" , \"COMMACATEGORY\" , \"DASHCATEGORY\" , \"ETCCATEGORY\" , \"EXCLAMATIONCATEGORY\" , \"PARENTHESISCATEGORY\" , \"PAUSECATEGORY\" , \"PERIODCATEGORY\" , \"QUESTIONCATEGORY\" , \"SEMICOLONCATEGORY\" , \"SPCHANGECATEGORY\" } all_senses_list = list () all_senses_list_sum = list () for i in range ( all_list . shape [ 0 ]): senses_list = list () arr = pd . Series ( word_sentence_list [ i ]) ttl = 0 for j in range ( len ( arr )): if ( pos_sentence_list [ i ][ j ] not in pun_set ): _word = arr [ j ] word = cwn . find_lemma ( \"^\" + _word + \"$\" ) sense_cnt = all_sense_tree ( word ) senses_list . append (( arr [ j ], pos_sentence_list [ i ][ j ], sense_cnt )) tp = sentence_list_type [ i ] year = sentence_list_year [ i ] all_senses_list . append (( tp , year , senses_list )) tmp_df = pd . DataFrame ( senses_list , columns = [ 'tagged_word' , 'CKIP_POS' , 'sense_cnt' ]) words_cnt = tmp_df . shape [ 0 ] tmp_df_ = tmp_df [ tmp_df [ 'sense_cnt' ] != 0 ] words_cnt_ = tmp_df_ . shape [ 0 ] all_senses_list_sum . append (( tp , year , words_cnt , tmp_df . sense_cnt . mean (), words_cnt_ , tmp_df_ . sense_cnt . mean ())) all_senses_df = pd . DataFrame ( all_senses_list , columns = [ 'type' , 'year' , 'tagged_words' ]) all_senses_df_sum = pd . DataFrame ( all_senses_list_sum , columns = [ 'type' , 'year' , 'words_cnt' , 'avg_sense_all' , 'words_cnt_nonzero' , 'avg_sense_nonzero' ]) all_senses_df_sum [ 'zero_sense_ratio' ] = all_senses_df_sum . words_cnt_nonzero . div ( words_cnt ) all_senses_df_sum = all_senses_df_sum . assign ( zero_sense_ratio = 1 - all_senses_df_sum [ 'zero_sense_ratio' ]) display ( all_senses_df_sum . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type year words_cnt avg_sense_all words_cnt_nonzero avg_sense_nonzero zero_sense_ratio 0 GSAT 105 399 5.100251 300 6.783333 0.353448 1 GSAT 107 268 5.313433 193 7.378238 0.584052 2 AST 101 345 6.191304 242 8.826446 0.478448 3 GSAT 101 345 6.011594 264 7.856061 0.431034 4 AST 103 394 5.581218 305 7.209836 0.342672","title":"\u55ae\u8a5e\u8a5e\u610f\u91cf\u8a08\u7b97"},{"location":"#histogram_1","text":"print ( \"average sense from all words = \" , all_senses_df_sum . avg_sense_all . mean ()) print ( \"average sense from non-zero-sensed words = \" , all_senses_df_sum . avg_sense_nonzero . mean ()) bins = np . linspace ( 4 , 11 , 35 ) plt . hist ( all_senses_df_sum . avg_sense_all , bins , alpha = 0.7 , label = 'avg sense - all words' ) plt . hist ( all_senses_df_sum . avg_sense_nonzero , bins , alpha = 0.7 , label = 'avg sense - non-zero-sensed words' ) plt . legend ( loc = 'upper right' , bbox_to_anchor = ( 1.7 , 1 )) plt . grid ( True ) plt . show () average sense from all words = 5.9117085939980205 average sense from non-zero-sensed words = 8.013096983316954 \u7d93\u904e\u8a08\u7b97\u5f8c\u53ef\u4ee5\u5f97\u77e5\u5728\u6240\u6709\u8cc7\u6599\u4e2d\uff0c\u5e73\u5747\u8fad\u610f\u70ba5.91\uff1b\u6263\u9664\u8fad\u610f\u70ba0\u7684\u55ae\u8a5e\u5f8c\u5e73\u5747\u8fad\u610f\u70ba8.01\u3002","title":"\u5e73\u5747\u8a5e\u610f\u91cfHistogram"},{"location":"#_9","text":"\u82e5\u6211\u5011\u5047\u8a2d\u8a5e\u610f\u91cf\u70ba\u96f6\u4e4b\u55ae\u8a5e\u662f\u51b7\u50fb\u8a5e\uff0c\u7d93\u904e\u8a08\u7b97\u53ef\u4ee5\u5f97\u5230\u6bcf\u7bc7\u6587\u7ae0\u4f7f\u7528\u51b7\u50fb\u8a5e\u7684\u6bd4\u7387\uff1a plot = all_senses_df_sum . hist ( column = 'zero_sense_ratio' ) print ( \"ratio mean = \" , all_senses_df_sum . zero_sense_ratio . mean ()) ratio mean = 0.3614161969496021 \u53ef\u898b\u5728\u6240\u6709\u8cc7\u6599\u96c6\u4e2d\uff0c\u51b7\u50fb\u5b57\u7684\u4f7f\u7528\u5e73\u5747\u6bd4\u4f8b\u70ba36.14%\u3002","title":"\u51b7\u50fb\u8a5e\u4f7f\u7528\u5206\u6790"},{"location":"#_10","text":"tmp_list = list () for i in range ( all_senses_df . shape [ 0 ]): tmp_list += all_senses_df . tagged_words [ i ] senses_df = pd . DataFrame ( tmp_list , columns = [ 'tagged_word' , 'CKIP_POS' , 'sense_cnt' ]) senses_df_ = senses_df [ senses_df [ 'CKIP_POS' ] != 'WHITESPACE' ] #delete \\n senses_df_ = senses_df_ [ senses_df_ [ 'tagged_word' ] != '\u3002 \\n ' ] #delete \u3002\\n #senses_df_.head() words_cnt_ = pd . DataFrame ( senses_df_ . value_counts ()) print ( \"Head of frequency of all words\" ) display ( words_cnt_ . head ( 10 )) words_cnt_Na = senses_df_ [ senses_df_ . CKIP_POS == 'Na' ] words_cnt_Na = pd . DataFrame ( words_cnt_Na . value_counts ()) print ( \" \\n\\n Head of frequency of Na(\u666e\u901a\u540d\u8a5e) words\" ) display ( words_cnt_Na . head ( 10 )) words_cnt_Nb = senses_df_ [ senses_df_ . CKIP_POS == 'Nb' ] words_cnt_Nb = pd . DataFrame ( words_cnt_Nb . value_counts ()) print ( \" \\n\\n Head of frequency of Nb(\u5c08\u6709\u540d\u7a31) words\" ) display ( words_cnt_Nb . head ( 10 )) words_cnt_V = senses_df_ [ senses_df_ . CKIP_POS != 'V_2' ] words_cnt_V = words_cnt_V . loc [ words_cnt_V [ 'CKIP_POS' ] . str . contains ( 'V' )] words_cnt_V = pd . DataFrame ( words_cnt_V . value_counts ()) print ( \" \\n\\n Head of frequency of V words (excluding V_2)\" ) display ( words_cnt_V . head ( 10 )) words_cnt_VA = senses_df_ [ senses_df_ . CKIP_POS == 'VA' ] words_cnt_VA = pd . DataFrame ( words_cnt_VA . value_counts ()) print ( \" \\n\\n Head of frequency of VA(\u52d5\u4f5c\u4e0d\u53ca\u7269\u52d5\u8a5e) words\" ) display ( words_cnt_VA . head ( 10 )) Head of frequency of all words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u7684 DE 16 7052 \u6211 Nh 3 2705 \u4e00 Neu 10 1454 \u662f SHI 9 1362 \u5728 P 10 1229 \u4e86 Di 5 836 \u4e0d D 3 699 \u8457 Di 8 652 \u81ea\u5df1 Nh 5 601 \u800c Cbb 10 569 Head of frequency of Na(\u666e\u901a\u540d\u8a5e) words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u4eba Na 11 481 \u5fc3 Na 13 247 \u751f\u547d Na 8 234 \u4eba\u751f Na 1 206 \u5922\u60f3 Na 0 107 \u5b78\u751f Na 2 86 \u9006\u5883 Na 0 83 \u5922 Na 3 81 \u90f5\u7b52 Na 0 78 \u751f\u6d3b Na 3 78 Head of frequency of Nb(\u5c08\u6709\u540d\u7a31) words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u5b54\u5b50 Nb 3 13 \u9805\u7fbd Nb 0 11 \u674e\u767d Nb 2 11 \u8607\u8fea\u52d2 Nb 0 10 \u8607\u8efe Nb 0 8 \u67f3\u5b97\u5143 Nb 0 8 \u67ef\u9ea5\u9686 Nb 0 8 \u9676\u6df5\u660e Nb 0 7 \u674e\u5b89 Nb 0 7 \u53f2\u8a18 Nb 1 6 Head of frequency of V words (excluding V_2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u8b93 VL 6 183 \u4f7f VL 13 181 \u8aaa VE 16 132 \u5927 VH 28 120 \u9762\u5c0d VC 2 119 \u6c92\u6709 VJ 6 107 \u60f3 VE 9 106 \u6df1 VH 17 88 \u770b VC 13 88 \u6109\u5feb VH 1 87 Head of frequency of VA(\u52d5\u4f5c\u4e0d\u53ca\u7269\u52d5\u8a5e) words .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 tagged_word CKIP_POS sense_cnt \u61c9\u8b8a VA 0 45 \u4f86 VA 19 44 \u901a\u95dc VA 0 39 \u5ea6\u4eba VA 0 36 \u5b58\u5728 VA 3 32 \u7b11 VA 2 30 \u6b6a\u8170 VA 0 29 \u7ad9 VA 7 23 \u8d70 VA 20 21 \u5750 VA 12 19 \u4ee5\u4e0a\u6211\u5011\u7d71\u8a08\u4e86\u5e7e\u500b\u7279\u5b9a\u8a5e\u6027\u5728\u6240\u6709\u8cc7\u6599\u96c6\u7684\u55ae\u8a5e\u51fa\u73fe\u983b\u7387\u3002","title":"\u7279\u5b9a\u8a5e\u6027\u55ae\u8a5e\u51fa\u73fe\u983b\u7387"},{"location":"#_11","text":"\u56de\u5230 R Project \u7814\u7a76\u7d50\u679c\u9801\u9762 \u56de\u5230 \u5165\u53e3\u9801\u9762","title":"\u9023\u7d50"}]}