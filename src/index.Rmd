---
title: "**台灣升大學考試國寫情意題佳作詞頻相關研究**"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: sandstone
---
**第三組 想不到組名**

**組員：朱修平、盧德原、楊舒晴、陳宛瑩**

<br>

<!-- PART1 -->
# **前言**
國寫在升大學考試之國文科目中，與選擇題各佔一半分數之比重，惟我國之國文教學，向來重視古文閱讀、國學常識等，在教學大綱中以選擇題的答寫作為主要教學方向，許多學生對於國寫部分之掌握能力相對不足。因此，本組希望透過分析國寫情意題佳作詞頻，進一步洞察國寫在我國命題方向與佳作取材等寫作方式。

---

<!-- PART2 -->
# **研究方法**

<br>

## **1資料來源與介紹**
本組參酌大考中心每年提供之升大學考試國文作文佳作範本，收錄範圍為民國95年至110年學測與98年至110年指考，共計208篇。另外，由於大考中心係提供範文手寫影本，資料前處理較為耗時，為免壓縮後續資料處理時間，106至110年之學測與指考皆各收錄兩筆資料，並以完整結構之作文為主（新式國寫有部分簡答題）。

## **2資料前處理**
本組以手動繕打將資料輸入電腦轉換成txt.檔，統一資料格式（例如：分段換行、無空格、完整標點符號、刪除底線、標準化檔名）後，再匯入Ｒ、python進行後續資料分析。
處理後之資料格式範例顯示如下：

------------- --------------------------------------------
file type     .txt

file name     GSAT_107_1

file content  四季遞嬗，各有各的紛繁絢麗，也各有各的落寞寂寥。然而，一個季節的組成絕不只是天氣上的變化，還要有繚繞的氣味，刻骨的畫面，以及活在季節的人們的故事，最好還要有一支如韋瓦第四季的曲子，使季節如電影般，有開演、有落幕。\\n
              閉上眼睛，常常想起的畫面通常背景是。
------------- --------------------------------------------


---

<!-- PART3 -->
# **資料分析**
本組將成果報告分成以下4個部分，依序為**基礎結構分析**、**引用資料分析**、**語彙詞頻分析**與**相似性分析**，詳細說明如下：

<!-- 所有會用到的library -->
```{r, echo = FALSE, warning=FALSE}
library(jiebaR, jiebaRD)
library(stringr)
library(dplyr)
library(ggplot2)
library (tidyverse)
library(tidytext)
library (ggpubr)
library(lubridate)
library(readr)
library(ggrepel)
library(ggthemr)
library("quanteda")
library("quanteda.textstats")
library("quanteda.textmodels")
library("magrittr")
```


<br>

## **1基礎結構分析**

### **1.1佳作文章長度**
<div class = "row">
<div class = "col-md-8">
```{r, echo = FALSE, message=FALSE, warning=FALSE}
text <- list.files(path = "../../data_set/", full.names = FALSE)## ÀÉ®×
paragraph_num <- vector("numeric", length = length(text))## number of paragraph
article_len <- vector("numeric", length = length(text))## length of article
sentence_num <- vector("numeric", length = length(text))## number of sentencce

post <- list()
for(i in seq_along(text)){
  post[[i]] <- readLines(paste0("../../data_set/", text[i]), encoding = "UTF-8", warn = FALSE)
  paragraph_num[i] <- length(post[[i]])
  a <- post[[i]][1]
  for(j in 2 : length(post[[i]])){
    a <- paste0(a, post[[i]][j])
  }
  post[[i]] <- a
  
}

post <- unlist(post)## turn to vector
for(i in seq_along(post)){ 
  sentence_num[i] <- length(unlist(strsplit(post[i], split = "[。！？]", fixed = FALSE)))
}


seg <- worker()
content <- vector("character", length(post))
for(i in seq_along(post)){
  segged <- segment(post[i], seg)
  article_len[i] <- nchar(paste(segged, collapse = ""))
  content[i] <- paste(segged, collapse = "\u3000")
}

df_break <- tibble::tibble(
  id = text,
  content = content
)

df_unbreak <- tibble::tibble(
  id = text,
  par_num = paragraph_num,
  sentence_num = sentence_num,
  article_len = article_len,
  content = post
)

ggthemr('dust')

#文章字數分佈圖
ggplot(data = df_unbreak) +
  geom_freqpoly(aes(x=article_len),size=1,color="#996666")+
  geom_vline(xintercept=549.75,color="grey",size=1)+
  geom_vline(xintercept=608,color="grey",size=0.5)+
  geom_vline(xintercept=677.25,color="grey",size=0.5)+
  labs(title="文章字數分佈密度圖",x="字數",y="篇數",caption ="*輔助線為四分位數（549.75,608,677.25)")+
  theme(text=element_text(family="LiHeiPro",size=12))
```
</div>
<div class = "col-md-4">
從該圖可以看出，佳作文章字數多介於600字上下，與從小學校教育要求的字數篇幅相吻合。


平均：615.21
標準差：94.16
中位數：607
眾數：650
</div>
</div>

### **1.2文章句數**
<div class = "row">
<div class = "col-md-8">
```{r, echo = FALSE, message=FALSE, warning=FALSE}
#文章句數分佈圖
ggplot(data = df_unbreak) +
  geom_freqpoly(aes(x=sentence_num),size=1,color="#996666")+
  geom_vline(xintercept=12,color="grey",size=1)+
  geom_vline(xintercept=15,color="grey",size=0.5)+
  geom_vline(xintercept=19,color="grey",size=0.5)+
  scale_shape_manual(values=c(4,15))+
  labs(title="文章句數分佈密度圖",x="句數",y="篇數",caption = "*輔助線為四分位數（12,15,19)")+
theme(text=element_text(family="LiHeiPro",size=12))
```
</div>
<div class = "col-md-4">
從該圖可以看出，佳作總句數分佈多介於15句上下。

平均：15.75
標準差：5.17
中位數：15
眾數：13
</div>
</div>

### **1.3段落分佈狀況**
<div class = "row">
<div class = "col-md-8">
```{r, echo = FALSE, warning=FALSE}
#文章段落數分佈圖
ggplot(data = df_unbreak) +
  geom_bar(aes(x=par_num),size=1,color="#996666")+
  labs(title="文章段落數分佈密度圖", x="段落數", y="篇數")+
 theme(text=element_text(family="LiHeiPro",size=12))
```
</div>
<div class = "col-md-4">
從上圖可以發現，多數文章仍是以四段式作文為主，多數佳作文章的寫作架構呈現課綱作文教學上之啟承轉合四段架構分配。

平均：4.91
標準差：1.5
中位數：4
眾數：4

</div>
</div>

<br>

## **2引用資料分析**
### **2.1名人偉人**
<div class = "row">
<div class = "col-md-6">
<br>
```{r, echo = FALSE, fig.height=6, fig.width=4, warning=FALSE}
year <- unlist(lapply(id, function(x){as.integer(strsplit(x, split = "_")[[1]][[2]])}))
docs_df$year <- year
year_df <- docs_df %>%
  group_by(year) %>%
  summarise(average = mean(name_times)) %>%
  arrange(year)

ggplot(data = year_df) +
  geom_bar(aes(x = reorder(year, -year), y = average), stat = "identity", size = 1, color = "#996666", show.legend = FALSE) +
  labs(title = "各年份佳作平均名人數", x = "年份", y = "名人數")+
  theme(text = element_text(size = 12), aspect.ratio = 1.5, plot.margin = margin(0, 0, 0, 0, "cm")) +
  coord_flip()+
  theme(text=element_text(family="LiHeiPro",size=12))
```
</div>

<div class = "col-md-6">
**歷年作文題目**

年份  學測題目           指考題目
----- ------------------ ----------------------
95年  雨季的故事
96年  走過
97年  如果當時...
98年  逆境               惑
99年  漂流木的獨白       應變
100年 學校和學生的關係   寬與深
101年 自勝者強           我可以終身奉行的一個字
102年 人間愉快           遠方
103年 通關密語           圓一個夢
104年 獨享               審己以度人
105年 我看歪腰郵筒       舉重若輕
106年 關於經驗的N種思考  在人際互動中找到自己
107年 季節的感思
108年 溫暖的心
109年 靜夜情懷
110年 如果我有一座新冰箱
----- ------------------ ----------------------
</div>
</div>

<br>

<div class = "row">

<div class = "col-md-8">
```{r, echo = FALSE, out.width='100%', warning=FALSE}
## 讀檔
id <- list.files("../../data_set/") # 文章的檔名
contents <- vector("character", length(id)) # 創一個長度與文章數相同的vector

# 將文章內容讀進來，並放入contents這個變數中(不分段)
for (i in seq_along(id)){
  path <- paste0("../../data_set/", id[i])
  contents[i] <- paste0(readLines(path, encoding = "UTF-8", warn=FALSE), collapse = "")
}

# 把檔名(id)和文章內容(contents)放進docs_df這個dataFrame中
docs_df <- tibble::tibble(
  id = id,
  content = contents
)

## 製圖

# 將放有偉人、名人名字的txt檔讀進來，存進names這個變數中
names <- readLines("../names.txt", encoding = "UTF-8", warn=FALSE)
names <- paste0(names, "、", collapse = "")
names <- unique(strsplit(names, split = "、")[[1]]) 

# 統計各篇文章出現幾個偉人、名人(若重複提到只計算一次)，儲存結果於docs_df的name_times欄位中
docs_df$name_times <- unlist(lapply(contents, function(x){sum(str_detect(x, names))}))

#製圖
ggthemr('dust')

docs_df1 <- docs_df %>% 
  group_by(name_times) %>%
  summarise(count =n(),percent =n()/208)
ggplot(docs_df1, aes(x=reorder(name_times,-count),y=count)) +
       geom_bar(width = 0.8, stat = "identity") +
   
#加上標題
labs(title="大考佳作出現名人次數分佈狀況",x="出現次數",y="篇數")+

#修改標籤名字
theme(text=element_text(size=12))+
geom_text(aes(label = count,y = -10),colour="black",size=4)+
geom_text(aes(label = paste0(sprintf("%1.1f", percent*100),"%"), y=(count+10)),size=6,position =position_dodge(0.2),colour="brown")+
  theme(text=element_text(family="LiHeiPro",size=12))
```
</div>

<div class = "col-md-4">

</div>
</div>

<br>

<div class = "row">
<div class = "col-md-7">
```{r, echo = FALSE, out.width='100%', fig.height=6, fig.width=5, warning=FALSE}
# 哪個名人、偉人出現在最多篇文章中
# 名字計數器(傳入某名字，函數會回傳提到此名字的文章有幾篇)
count_times <- function(name){
  count <- 0 
  for(i in seq_along(contents)){
    if (str_detect(contents[[i]], name) == TRUE){
      count <- count + 1
    }
  }
  return (count)
}

# 計算每個名字各出現在幾篇文章中
article_found_Q <- unlist(lapply(names, count_times))

# 出現某名人的文章是哪幾篇
find_article <- function(name){
  article_vec <- c()
  for (i in seq_along(contents)){
    if (str_detect(contents[[i]], name) == TRUE){
        article_vec <- c(article_vec, id[[i]])
    }
  }
  return(paste0(article_vec, " ", collapse = ""))
}
article_found <- unlist(lapply(names, find_article))

# 將名字與篇數放入names_df中
names_df <- tibble::tibble(
  name = names,
  article_found_Q = article_found_Q,
  article_found = article_found
  )

## 圖
ggthemr('dust')

#製圖
names_df1 <- names_df %>%
  filter(article_found_Q >= 4)

ggplot(names_df1, aes(x = reorder(name, article_found_Q), y = article_found_Q)) +
  geom_bar(width = 0.8, stat = "identity") +
  #加上標題
  labs(title = "名人出現次數排行", x = "人名", y = "篇數") +
  #修改標籤名字
  theme(text = element_text(size = 12), aspect.ratio = 1.5) +
  geom_text(aes(label = article_found_Q, y=(article_found_Q+1)),size=4,position =position_dodge(0.2),colour="brown") +
  scale_y_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10,11)) +
  coord_flip()+
  theme(text=element_text(family="LiHeiPro",size=12))
```
</div>
<div class = "col-md-5">
<br>
從該圖呈現之結果可以看出，佳作文章中的名人出現頻率以中國古人為主，其中又以孔子出現次數最多，又上述出現之中國古人物，皆屬課綱30古文之作者。
</div>
</div>

### **2.2佳句**
<div class = "row">
<div class = "col-md-7">
```{r, echo = FALSE, out.width='100%', fig.height=5, fig.width=5, warning=FALSE, message=FALSE}
# 有引用佳句的文章比例為多少
df_good_sent <- tibble::tibble(id = id)
df_quote <- df_good_sent %>%
  mutate(quote = (id %in% good_sent_id)) %>%
  group_by(quote) %>%
  summarise(quote_Q = n(), percentage = n()/208)

#製圖
ggplot(df_quote, aes(x=" ", y=percentage, fill=quote)) +
       geom_bar(width = 1, stat = "identity") +
       coord_polar("y", start=0)+ 
  geom_text(aes(label = quote_Q), position = position_stack(vjust = 0.5),size=9)+
                
#加上標題
labs(title="引用佳句之文章比例", x = "", y = "", fill = "")+

#修改標籤名字
theme(text=element_text(family="LiHeiPro",size=12))

```
</div>
<div class = "col-md-5">
<br>
<br>
從左圖可以看出，有引用佳句之文章約有6成，超過半數，此外可以發現若該年度作文主題非典型之抒情、敘事文，佳句引用比例有較高的趨勢。
</div>
</div>

**佳句集錦**
```{r, echo=FALSE, attr.output='style="max-height: 300px;"', warning=FALSE}
good_sent <- c()
good_sent_id <- c()

# 含冒號
for (i in seq_along(id)){
  sentences <- unique(str_match_all(contents[[i]], "：(\\w|，){5,}(。|？|！)")[[1]][,1])
  good_sent <- c(good_sent, sentences)
  good_sent_id <- c(good_sent_id, rep(id[i], length(sentences)))
}

# 含引號
for (i in seq_along(id)){
  sentences <- unique(str_match_all(contents[[i]], "「(\\w|，|。|！|；|、){5,}?」")[[1]][,1])
  good_sent <- c(good_sent, sentences)
  good_sent_id <- c(good_sent_id, rep(id[i], length(sentences)))
}

# 拿掉冒號和上下引號
good_sent <- unlist(lapply(good_sent, function(x){str_replace_all(x, "[：「」]", "")}))


# 人工移除不合適結果
good_sent <- good_sent[-c(5, 6, 7, 8, 9, 10, 11, 13, 18, 19, 20, 22, 24, 25, 26, 27, 28, 32, 35, 40, 43, 44, 58, 61, 73, 74, 75, 76, 83, 85, 91, 95, 96, 97, 102, 103, 104, 105, 109, 115, 118, 119, 122, 124, 126, 128, 132, 135, 137, 138, 143, 146, 148, 150, 159, 160, 163, 169, 171, 174, 175, 176, 189, 193, 194, 197, 203, 208, 213, 216, 219, 221, 223, 232, 235, 242, 243, 245, 258, 259, 268, 273, 274, 275, 276, 278, 288, 304, 306, 309, 312, 314, 315, 316, 323, 324, 329, 330, 336, 345, 346, 347)]

good_sent_id <- good_sent_id[-c(5, 6, 7, 8, 9, 10, 11, 13, 18, 19, 20, 22, 24, 25, 26, 27, 28, 32, 35, 40, 43, 44, 58, 61, 73, 74, 75, 76, 83, 85, 91, 95, 96, 97, 102, 103, 104, 105, 109, 115, 118, 119, 122, 124, 126, 128, 132, 135, 137, 138, 143, 146, 148, 150, 159, 160, 163, 169, 171, 174, 175, 176, 189, 193, 194, 197, 203, 208, 213, 216, 219, 221, 223, 232, 235, 242, 243, 245, 258, 259, 268, 273, 274, 275, 276, 278, 288, 304, 306, 309, 312, 314, 315, 316, 323, 324, 329, 330, 336, 345, 346, 347)]

# 佳句
good_sent

```


**重複出現之佳句**
```{r, echo=FALSE, warning=FALSE}
# 去除所有標點符號
good_sent_clean <- unlist(lapply(good_sent, function(x){str_replace_all(x, "[，。！、]", "")}))

# 計算相似度
simhasher = worker("simhash", topn = 10)
for(i in 1:(length(good_sent)-1)){
  for(j in((i+1):length(good_sent))){
    sim <- distance(good_sent_clean[i], good_sent_clean[j], simhasher)$distance
    if (sim < 12){
      if (!(good_sent_id[i] == good_sent_id[j])){
        cat(good_sent_id[i], good_sent[i], "\n")
        cat(good_sent_id[j], good_sent[j], "\n")
        cat("\n")
      }
    }
  } 
}
```

<br>

## **3相似性分析**
### **3.1首段與尾段落相似性**
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library("jiebaR", "jiebaRD")
library("stringr")
library("dplyr")
library("quanteda")
library("quanteda.textstats")
library("quanteda.textmodels")
library("magrittr")
library("readxl")
eudist <- function(x1, x2) 
  sqrt( sum( (as_unit_vec(x1) - as_unit_vec(x2))^2 ) )
cossim <- function(x1, x2)
  sum(x1 * x2) / sqrt( sum(x1^2) * sum(x2^2) )
as_unit_vec <- function(x) x / sqrt(sum(x^2))

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
# library("reticulate")
# use_python("D:\\Anaconda")
# py_available()


text <- list.files(path = "../../data_set/", full.names = FALSE)## ÀÉ®×
paragraph_num <- vector("numeric", length = length(text))## number of paragraph
article_len <- vector("numeric", length = length(text))## length of article
sentence_num <- vector("numeric", length = length(text))## number of sentencce

post <- list()
first <- list()
last <- list()
for(i in seq_along(text)){
  post[[i]] <- readLines(paste0("../../data_set/", text[i]), encoding = "UTF-8", warn = FALSE)
  paragraph_num[i] <- length(post[[i]])
  a <- post[[i]][1]
  first[i] <- post[[i]][1]
  last[i] <- post[[i]][length(post[[i]])]
  for(j in 2 : length(post[[i]])){
    a <- paste0(a, post[[i]][j])
  }
  post[[i]] <- a
}

first <- unlist(first)
last <- unlist(last)
post <- unlist(post)## turn to vector
for(i in seq_along(post)){ 
  sentence_num[i] <- length(unlist(strsplit(post[i], split = "[。！？]", fixed = FALSE)))
}

# print(last)
seg <- worker()
content <- vector("character", length(post))
first_break <- vector("character", length(post))
last_break <- vector("character", length(post))
for(i in seq_along(post)){
  segged <- segment(post[i], seg)
  segged_F <- segment(first[i], seg)
  segged_L <- segment(last[i], seg)
  article_len[i] <- nchar(paste(segged, collapse = ""))
  content[i] <- paste(segged, collapse = "\u3000")
  first_break[i] <- paste(segged_F, collapse = "\u3000")
  last_break[i] <- paste(segged_L, collapse = "\u3000")
}
similar <- vector("numeric", length(post))
for(i in seq_along(post)){
  a <- c(first_break[i], last_break[i])
  # print(a)
  quanteda_corpus_a <- corpus(a) %>%
    tokenizers::tokenize_regex(pattern =  "\u3000") %>%
    tokens()
  q_dfm <- dfm(quanteda_corpus_a) %>%
    dfm_remove(pattern =  readLines("../stopwords.txt"), valuetype = "fixed") %>%
    dfm_select(pattern = "[\u4E00-\u9FFF]", valuetype = "regex") %>%
    dfm_tfidf()
  similar[i] <- cossim(q_dfm[1, ], q_dfm[2, ])
}
print(similar)

df_break <- tibble::tibble(
  id = text,
  content = content
)

df_unbreak <- tibble::tibble(
  id = text,
  par_num = paragraph_num,
  sentence_num = sentence_num,
  article_len = article_len,
  content = post
)



```
### **3.2輸入新文章分析器**
```{r, echo=FALSE, warning=FALSE, message=FALSE}

library("jiebaR", "jiebaRD")
library("stringr")
library("dplyr")
library("quanteda")
library("quanteda.textstats")
library("quanteda.textmodels")
library("magrittr")
library("readxl")
eudist <- function(x1, x2) 
  sqrt( sum( (as_unit_vec(x1) - as_unit_vec(x2))^2 ) )
cossim <- function(x1, x2)
  sum(x1 * x2) / sqrt( sum(x1^2) * sum(x2^2) )
as_unit_vec <- function(x) x / sqrt(sum(x^2))

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
# library("reticulate")
# use_python("D:\\Anaconda")
# py_available()

text <- list.files(path = "../../data_set/", full.names = FALSE)## ÀÉ®×
paragraph_num <- vector("numeric", length = length(text))## number of paragraph
article_len <- vector("numeric", length = length(text))## length of article
sentence_num <- vector("numeric", length = length(text))## number of sentencce

post <- list()
first <- list()
last <- list()
for(i in seq_along(text)){
  post[[i]] <- readLines(paste0("../../data_set/", text[i]), encoding = "UTF-8", warn = FALSE)
  paragraph_num[i] <- length(post[[i]])
  a <- post[[i]][1]
  first[i] <- post[[i]][1]
  last[i] <- post[[i]][length(post[[i]])]
  for(j in 2 : length(post[[i]])){
    a <- paste0(a, post[[i]][j])
  }
  post[[i]] <- a
}

first <- unlist(first)
last <- unlist(last)
post <- unlist(post)## turn to vector
for(i in seq_along(post)){ 
  sentence_num[i] <- length(unlist(strsplit(post[i], split = "[。！？]", fixed = FALSE)))
}

# print(last)
seg <- worker()
content <- vector("character", length(post))
first_break <- vector("character", length(post))
last_break <- vector("character", length(post))
for(i in seq_along(post)){
  segged <- segment(post[i], seg)
  segged_F <- segment(first[i], seg)
  segged_L <- segment(last[i], seg)
  article_len[i] <- nchar(paste(segged, collapse = ""))
  content[i] <- paste(segged, collapse = "\u3000")
  first_break[i] <- paste(segged_F, collapse = "\u3000")
  last_break[i] <- paste(segged_L, collapse = "\u3000")
}
# similar <- vector("numeric", length(post))
# for(i in seq_along(post)){
#   a <- c(first_break[i], last_break[i])
#   quanteda_corpus_a <- corpus(a) %>%
#     tokenizers::tokenize_regex(pattern =  "\u3000") %>%
#     tokens()
#   q_dfm <- dfm(quanteda_corpus_a) %>%
#     dfm_remove(pattern =  readLines("../stopwords.txt"), valuetype = "fixed") %>%
#     dfm_select(pattern = "[\u4E00-\u9FFF]", valuetype = "regex") %>%
#     dfm_tfidf()
#   similar[i] <- eudist(q_dfm[1,], q_dfm[2,])
# }
# 
# print(similar)

df_break <- tibble::tibble(
  id = text,
  content = content
)

df_unbreak <- tibble::tibble(
  id = text,
  par_num = paragraph_num,
  sentence_num = sentence_num,
  article_len = article_len,
  content = post
)

# df_break
# df_unbreak
# 
df <- read_excel("../frequency_30_word.xlsx")
test <- unname(unlist(as.list(df["1"])))

quanteda_corpus <- corpus(df_break, 
                          docid_field = "id", 
                          text_field = "content") %>%
  tokenizers::tokenize_regex(pattern =  "\u3000") %>%
  tokens()

q_dfm <- dfm(quanteda_corpus) %>% 
  dfm_select(test) %>%
  # dfm_remove(pattern =  readLines("../stopwords.txt"), valuetype = "fixed") %>%
  dfm_tfidf()
q_dfm

doc_sim <- textstat_simil(q_dfm, method = "cosine") %>% as.matrix()
sort(doc_sim["AST_100_1.txt", ], decreasing = T)[1:20]
# a <- c(71.42, 41.66, 50.4, 60.6, 100, 100, 60, 38.4, 42.8, 79.5, 60.6, 57.7, 100, 39.2, 75)## above 50%
# b <- c(50, 50, 70.5, 90.9, 100, 100, 60, 46.1, 42.8, 63.6, 90.9, 86.6, 100, 55, 75)




```

<br>

## **4語彙詞頻分析**
**由於此部分程式以Python撰寫，詳細結果請參閱以下網頁** [連結]( https://derekdylu.github.io/LING5505-Final-Project-Group3/source/site/sense.html)

### **4.1中文語彙詞頻**
### **4.2冷僻字使用佔比**

---

# **小結**
從上述結果可以看出，佳作文章與傳統作文寫作方向並無太大差異，在結構上多為600字上下之作文，以四段式作文為主，有超過六成之文章引用名言佳句，而在文章中最常出現的偉人以中國古文人為主，又以孔子出現頻率最高。此外，在文章相似性部分，只要同屆的作文篇幅超過5篇，能在相似度大於50%下找到同屆作文的比率超過70%。最後，我們可以從詞頻相關的研究結果中看出，綜合所有文章觀察的結果，平均詞意量約 6個詞、動詞名詞比例約為 1.5、
佳作冷僻詞使用量約 36%。

---

